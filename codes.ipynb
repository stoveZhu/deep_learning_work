{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15835\\anaconda3\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated and will be removed in a future release\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import jieba\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import math\n",
    "\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fact</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_0</td>\n",
       "      <td>罪犯陈中盛，男，1954年xx月xx日出生，汉族，湖南省永兴县人，文盲，现在河南省新郑监狱服...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_1</td>\n",
       "      <td>罪犯王中国，男，1982年xx月xx日生，汉族，吉林省长春市宽城区人，初中毕业，现在吉林省长...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_2</td>\n",
       "      <td>罪犯张仁奇，现在宁夏回族自治区银川监狱服刑，以被告人张仁奇犯信用卡诈骗罪，判处有期徒刑刑期六...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_3</td>\n",
       "      <td>罪犯吴晨，男，1988年xx月xx日出生，瑶族，广西巴马县人，初中文化，原住广西巴马县巴马镇...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_4</td>\n",
       "      <td>罪犯夏宏，女，1980年xx月xx日出生，汉族，初中文化，湖南省邵阳市大祥区人，住湖南省邵阳...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               fact  label\n",
       "0  id_0  罪犯陈中盛，男，1954年xx月xx日出生，汉族，湖南省永兴县人，文盲，现在河南省新郑监狱服...     12\n",
       "1  id_1  罪犯王中国，男，1982年xx月xx日生，汉族，吉林省长春市宽城区人，初中毕业，现在吉林省长...     13\n",
       "2  id_2  罪犯张仁奇，现在宁夏回族自治区银川监狱服刑，以被告人张仁奇犯信用卡诈骗罪，判处有期徒刑刑期六...      8\n",
       "3  id_3  罪犯吴晨，男，1988年xx月xx日出生，瑶族，广西巴马县人，初中文化，原住广西巴马县巴马镇...      7\n",
       "4  id_4  罪犯夏宏，女，1980年xx月xx日出生，汉族，初中文化，湖南省邵阳市大祥区人，住湖南省邵阳...      6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#提取源数据\n",
    "raw_data = pd.read_csv('train.csv')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#清洗数据\n",
    "\n",
    "\n",
    "def text_preprocess(text):\n",
    "    text = re.sub(r'<.*?>', '', text)#HTML标签\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)#特殊符号\n",
    "    text = text.replace(' ', '')#空格\n",
    "\n",
    "    with open('hit_stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "        stopwords = [line.strip() for line in f.readlines()]\n",
    "    text = jieba.cut(text)\n",
    "    text = ' '.join([word for word in text if word not in stopwords])\n",
    "    pattern = re.compile(r'([\\u4e00-\\u9fa5]{2,5}?(?:省|自治区|市)){0,1}([\\u4e00-\\u9fa5]{2,7}?(?:区|县|州)){0,1}([\\u4e00-\\u9fa5]{2,7}?(?:镇)){0,1}([\\u4e00-\\u9fa5]{2,7}?(?:村|街|街道)){0,1}([\\d]{1,3}?(号)){0,1}')\n",
    "    text =pattern.sub( '', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\15835\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.431 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "clean_list = [text_preprocess(text) for text in raw_data['fact'].to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['clean_fact'] = clean_list\n",
    "raw_data['fact_len'] = [len(i.split()) for i in clean_list]\n",
    "raw_data['fact_len'].describe(percentiles=[.5,.95])\n",
    "raw_data.to_csv('processed_data.csv')\n",
    "X_train, X_test, y_train, y_test = train_test_split(raw_data.clean_fact, raw_data.label, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(vector_size=400,\n",
    "               min_count=10)    \n",
    " \n",
    "w2v.build_vocab(X_train)\n",
    "w2v.train(X_train,total_examples=w2v.corpus_count,epochs=40)\n",
    "w2v.save('./word2vec/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10001"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv('processed_data.csv')\n",
    "X_train, X_test, y_train, y_test = train_test_split(raw_data.clean_fact, raw_data.label.values, test_size=0.2, random_state=42, shuffle=False)\n",
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(raw_data.clean_fact, raw_data.label.values, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "def average_vec(text):\n",
    "    vec = np.zeros(400).reshape((1, 400))\n",
    "    for word in text:\n",
    "        try:\n",
    "            vec += w2v.wv[word].reshape((1, 400))\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return vec\n",
    "\n",
    "w2v = Word2Vec.load(\"word2vec\\word2vec.model\")\n",
    "X_train_vec = np.concatenate([average_vec(z) for z in X_train])\n",
    "X_test_vec = np.concatenate([average_vec(z) for z in X_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.labels = label\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(sample, dtype=torch.float32).unsqueeze(0), torch.tensor(label, dtype=torch.float)\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2V_CNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, conv_kernel_size, maxpool_kernel_size, stride, padding, linear_input):\n",
    "        super(W2V_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels = input_dim, out_channels = 16, kernel_size = conv_kernel_size, stride = stride, padding = padding)\n",
    "        self.conv2 = nn.Conv1d(in_channels = 16, out_channels = 32, kernel_size = conv_kernel_size, stride = stride, padding = padding)\n",
    "        self.max_pool = nn.MaxPool1d(kernel_size = maxpool_kernel_size) \n",
    "        self.l1 = nn.Linear(linear_input, 1024)\n",
    "        self.l2 = nn.Linear(1024,256)\n",
    "        self.l3 = nn.Linear(256,output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ####### Your code between this #########\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        #print (x.size())\n",
    "        x = x.view(-1,32*100)\n",
    "        x = self.l1(self.relu(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.l2(self.relu(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.l3(self.relu(x))\n",
    "        x = F.log_softmax(x,dim=1)\n",
    "        #self.out(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "input_dim = 1\n",
    "output_dim = 34\n",
    "conv_kernel_size = 3\n",
    "maxpool_kernel_size = 2\n",
    "stride = 1\n",
    "padding = 1\n",
    "linear_input = 32*100\n",
    "\n",
    "LR = 1e-3\n",
    "EPOCHS = 30\n",
    "\n",
    "train_dataset = myDataset(X_train_vec, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=9192, shuffle=True)\n",
    "model = W2V_CNN(input_dim, output_dim, conv_kernel_size, maxpool_kernel_size, stride, padding, linear_input).to(device)\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 299.1339\n",
      "Epoch [2/30], Loss: 65.4407\n",
      "Epoch [3/30], Loss: 17.1878\n",
      "Epoch [4/30], Loss: 15.7820\n",
      "Epoch [5/30], Loss: 14.8887\n",
      "Epoch [6/30], Loss: 14.3976\n",
      "Epoch [7/30], Loss: 14.1127\n",
      "Epoch [8/30], Loss: 13.9519\n",
      "Epoch [9/30], Loss: 13.7249\n",
      "Epoch [10/30], Loss: 13.5634\n",
      "Epoch [11/30], Loss: 13.3652\n",
      "Epoch [12/30], Loss: 13.2359\n",
      "Epoch [13/30], Loss: 13.0493\n",
      "Epoch [14/30], Loss: 12.9706\n",
      "Epoch [15/30], Loss: 12.8262\n",
      "Epoch [16/30], Loss: 12.7004\n",
      "Epoch [17/30], Loss: 12.6004\n",
      "Epoch [18/30], Loss: 12.4972\n",
      "Epoch [19/30], Loss: 12.4179\n",
      "Epoch [20/30], Loss: 12.3905\n",
      "Epoch [21/30], Loss: 12.2804\n",
      "Epoch [22/30], Loss: 12.2515\n",
      "Epoch [23/30], Loss: 12.1636\n",
      "Epoch [24/30], Loss: 12.1020\n",
      "Epoch [25/30], Loss: 12.0269\n",
      "Epoch [26/30], Loss: 11.9755\n",
      "Epoch [27/30], Loss: 11.9315\n",
      "Epoch [28/30], Loss: 11.8673\n",
      "Epoch [29/30], Loss: 11.8438\n",
      "Epoch [30/30], Loss: 11.7743\n",
      "Traning finished !\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        ####### Your code between this #########\n",
    "        output = model(inputs)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        ####### Your code between this #########\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: {running_loss:.4f}')\n",
    "print(\"Traning finished !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'./models/W2V_CNN.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = torch.load('./models/W2V_CNN.model').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Score1(pred,true):\n",
    "    score = 0\n",
    "    v = torch.abs(torch.log(pred+1)-torch.log(true+1))\n",
    "    for i in v:\n",
    "        if i.item()<=0.2:\n",
    "            score += 1\n",
    "        elif i.item() <= 0.4:\n",
    "            score+=0.8\n",
    "        elif i.item() <=0.6:\n",
    "            score+=0.6\n",
    "        elif i.item() <=0.8:\n",
    "            score += 0.4\n",
    "        elif i.item() <=1:\n",
    "            score += 0.2\n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtAcc:0.24350247524752475,Score1:132090.00000000003,FinalScore:0.6340037128712872\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "test_dataset = myDataset(X_test_vec,y_test.to_list())\n",
    "test_loader = DataLoader(test_dataset,shuffle=True,batch_size=200)\n",
    "\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "score1 = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(torch.exp(outputs.data), 1)\n",
    "        total += labels.size(0)\n",
    "        score1 += Score1(predicted,labels)\n",
    "        correct += (predicted == labels).sum().item()   \n",
    "\n",
    "ExtAcc = correct/total\n",
    "\n",
    "print(f\"ExtAcc:{ExtAcc},Score1:{score1},FinalScore:{0.3*ExtAcc+0.7*score1/total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data(df, tokenizer, max_length):\n",
    "    inputs = tokenizer(\n",
    "        df['fact'].tolist(),\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return inputs\n",
    "\n",
    "# 使用BERT tokenizer进行文本编码\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "train_inputs = preprocess_data(raw_data, tokenizer, max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_classes, kernel_sizes, num_channels):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_channels, (k, embed_size)) for k in kernel_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_channels, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)  # (batch_size, 1, seq_len, embed_size)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]  # [(batch_size, num_channels, seq_len), ...]\n",
    "        x = [F.max_pool1d(item, item.size(2)).squeeze(2) for item in x]  # [(batch_size, num_channels), ...]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc(x)\n",
    "        return F.log_softmax(logits,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = raw_data['label'].values\n",
    "\n",
    "X_train, X_val, attention_mask_train, attention_mask_val, y_train, y_val = train_test_split(\n",
    "    train_inputs['input_ids'], train_inputs['attention_mask'], train_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.inputs.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "    \n",
    "train_inputs_train_dict = {'input_ids': X_train, 'attention_mask': attention_mask_train}\n",
    "train_inputs_val_dict = {'input_ids': X_val, 'attention_mask': attention_mask_val}\n",
    "\n",
    "train_dataset = CustomDataset(train_inputs_train_dict, y_train)\n",
    "val_dataset = CustomDataset(train_inputs_val_dict, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "embed_size = 512\n",
    "num_classes = 34\n",
    "kernel_sizes = [3, 4, 5]\n",
    "num_channels = 100\n",
    "\n",
    "model = TextCNN(vocab_size, embed_size, num_classes, kernel_sizes, num_channels)\n",
    "\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/10: 100%|██████████| 40/40 [00:22<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 147.7819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/10: 100%|██████████| 40/40 [00:22<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 132.5046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/10: 100%|██████████| 40/40 [00:22<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 128.6599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/10: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 125.7157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/10: 100%|██████████| 40/40 [00:22<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 123.3293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/10: 100%|██████████| 40/40 [00:22<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 121.3128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/10: 100%|██████████| 40/40 [00:22<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 119.5348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/10: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 118.1221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/10: 100%|██████████| 40/40 [00:22<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 116.7963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/10: 100%|██████████| 40/40 [00:22<00:00,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 115.4546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_predictions = []\n",
    "    train_labels_list = []\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_predictions.extend(outputs.cpu().detach().numpy())\n",
    "        train_labels_list.extend(labels.cpu().detach().numpy())\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'./models/TextCNN.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 10/10: 100%|██████████| 10/10 [00:02<00:00,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtAcc:0.23097690230976903,Score1:7673.599999999989,FinalScore:0.6063913608639128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "score1 = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\"):\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(torch.exp(outputs.data), 1)\n",
    "        total += labels.size(0)\n",
    "        score1 += Score1(predicted,labels)\n",
    "        correct += (predicted == labels).sum().item()   \n",
    "\n",
    "ExtAcc = correct/total\n",
    "\n",
    "print(f\"ExtAcc:{ExtAcc},Score1:{score1},FinalScore:{0.3*ExtAcc+0.7*score1/total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRNN_Att(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size,hidden_size,num_layers,hidden_size2,num_classes):\n",
    "        super(TextRNN_Att, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers,\n",
    "                            bidirectional=True, batch_first=True, dropout=0.5)\n",
    "        self.tanh1 = nn.Tanh()\n",
    "        # self.u = nn.Parameter(torch.Tensor(config.hidden_size * 2, config.hidden_size * 2))\n",
    "        self.w = nn.Parameter(torch.zeros(hidden_size * 2))\n",
    "        self.tanh2 = nn.Tanh()\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, hidden_size2)\n",
    "        self.fc = nn.Linear(hidden_size2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x\n",
    "        emb = self.embedding(x)  # [batch_size, seq_len, embeding]=[128, 32, 300]\n",
    "        H, _ = self.lstm(emb)  # [batch_size, seq_len, hidden_size * num_direction]=[128, 32, 256]\n",
    "\n",
    "        M = self.tanh1(H)  # [128, 32, 256]\n",
    "        # M = torch.tanh(torch.matmul(H, self.u))\n",
    "        alpha = F.softmax(torch.matmul(M, self.w), dim=1).unsqueeze(-1)  # [128, 32, 1]\n",
    "        out = H * alpha  # [128, 32, 256]\n",
    "        out = torch.sum(out, 1)  # [128, 256]\n",
    "        out = F.relu(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc(out)  # [128, 64]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "embed_size = 512\n",
    "num_classes = 34\n",
    "hidden_size = 128                                       \n",
    "num_layers = 2                                            \n",
    "hidden_size2 = 64\n",
    "\n",
    "model = TextRNN_Att(vocab_size, embed_size,hidden_size,num_layers,hidden_size2,num_classes=num_classes)\n",
    "\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adagrad(params=model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/10: 100%|██████████| 40/40 [03:40<00:00,  5.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 113.5696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/10: 100%|██████████| 40/40 [03:37<00:00,  5.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 104.8152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/10: 100%|██████████| 40/40 [03:44<00:00,  5.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 97.6506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/10: 100%|██████████| 40/40 [05:06<00:00,  7.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 93.5851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/10: 100%|██████████| 40/40 [04:57<00:00,  7.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 90.5896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/10: 100%|██████████| 40/40 [04:53<00:00,  7.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 88.1664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/10: 100%|██████████| 40/40 [04:56<00:00,  7.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 85.9660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/10: 100%|██████████| 40/40 [04:59<00:00,  7.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 84.0526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/10: 100%|██████████| 40/40 [05:02<00:00,  7.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 81.9910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/10: 100%|██████████| 40/40 [05:12<00:00,  7.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 79.8581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_predictions = []\n",
    "    train_labels_list = []\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids']\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_predictions.extend(outputs.cpu().detach().numpy())\n",
    "        train_labels_list.extend(labels.cpu().detach().numpy())\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 10/10: 100%|██████████| 10/10 [00:15<00:00,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtAcc:0.3845615438456154,Score1:8870.799999999985,FinalScore:0.7362623737626226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "score1 = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\"):\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(torch.exp(outputs.data), 1)\n",
    "        total += labels.size(0)\n",
    "        score1 += Score1(predicted,labels)\n",
    "        correct += (predicted == labels).sum().item()   \n",
    "\n",
    "ExtAcc = correct/total\n",
    "\n",
    "print(f\"ExtAcc:{ExtAcc},Score1:{score1},FinalScore:{0.3*ExtAcc+0.7*score1/total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"./models/TxtxRNN_Att.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 3126/3126 [00:21<00:00, 145.62it/s]\n"
     ]
    }
   ],
   "source": [
    "testA = pd.read_csv('testA.csv')\n",
    "test_inputs = preprocess_data(testA, tokenizer, max_length=512)\n",
    "test_inputs_train_dict = {'input_ids': test_inputs['input_ids'], 'attention_mask': test_inputs['attention_mask']}\n",
    "labels = [0]*len(testA)\n",
    "\n",
    "dataset = CustomDataset(test_inputs_train_dict,labels)\n",
    "test_loader_A = DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "def predict(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            outputs = model(input_ids)\n",
    "            _,predict= torch.max(torch.exp(outputs.data), 1)\n",
    "            predictions.extend(predict.cpu().numpy())\n",
    "    return predictions\n",
    "\n",
    "predictions_A = predict(model, test_loader_A)\n",
    "submission =pd.DataFrame()\n",
    "submission['id']=testA.id\n",
    "submission['label']=predictions_A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"mySubmission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
